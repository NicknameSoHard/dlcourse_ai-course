{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rks8q8M6uH5o",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbAVrSAJUyKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\") # Let's make sure GPU is available!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "a42ae50a-7afd-46a9-c143-6ac9a18ef31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "ac5046b8-04f7-480e-8171-b21a6f0ea04e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "c075e999-a0c5-4266-be26-36729bc2f8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "c92dd8c7-b67b-44d5-e38c-7016cef39828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'ADP', 'NUM', 'PRON', 'X', '.', 'DET', 'ADV', 'CONJ', 'PRT', 'ADJ', 'NOUN', 'VERB'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "6ea0a5fd-5984-4172-dd90-ac8f1c555c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "5bfee63e-04ef-4167-bca1-3585c55126dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "97b4d956-728c-4b49-b2bc-a85f2f877615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "aebb87f7-7c5c-4b63-e8c5-0a6f7800301f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "c8a83119-61b3-4eb8-f40b-15590365371d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #print(\"Inputs: \", inputs.shape)\n",
        "        emb = self._emb(inputs)\n",
        "        #print(\"Emb: \", emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        #print(\"Output of LSTM: \", output.shape)\n",
        "        out = self._out_layer(output)\n",
        "        #print(\"Final output\", out.shape)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "4b628fef-77fb-48aa-afba-d6dd202dc5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "mask = (y_batch != 0).float()\n",
        "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "total_count = mask.sum().item()\n",
        "\n",
        "correct_count / total_count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07608695652173914"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "18159db3-d028-421b-eb70-3ee3190bc700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).type(torch.cuda.FloatTensor)\n",
        "logits = model(X_batch)\n",
        "loss = 0\n",
        "for ind, row in enumerate(logits):\n",
        "    loss += criterion(row, y_batch[ind])\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(82.2195, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = 0\n",
        "                for ind, row in enumerate(logits):\n",
        "                    loss += criterion(row, y_batch[ind])\n",
        "                \n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                \n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                mask = (y_batch != 0).float()\n",
        "                \n",
        "                cur_correct_count, cur_sum_count = ((preds == y_batch).float() * mask).sum().item(), mask.sum().item()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "68face1a-dc9d-4a24-cf35-f8741b2f9474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    lstm_layers_count=2,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 27.65614, Accuracy = 85.24%: 100%|██████████| 572/572 [00:10<00:00, 54.54it/s]\n",
            "[1 / 50]   Val: Loss = 22.47079, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 68.27it/s]\n",
            "[2 / 50] Train: Loss = 9.34165, Accuracy = 94.71%: 100%|██████████| 572/572 [00:10<00:00, 55.73it/s]\n",
            "[2 / 50]   Val: Loss = 18.19105, Accuracy = 93.76%: 100%|██████████| 13/13 [00:00<00:00, 62.41it/s]\n",
            "[3 / 50] Train: Loss = 6.43631, Accuracy = 95.85%: 100%|██████████| 572/572 [00:10<00:00, 56.04it/s]\n",
            "[3 / 50]   Val: Loss = 17.27299, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 66.84it/s]\n",
            "[4 / 50] Train: Loss = 5.43016, Accuracy = 96.33%: 100%|██████████| 572/572 [00:10<00:00, 55.96it/s]\n",
            "[4 / 50]   Val: Loss = 15.32912, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 67.34it/s]\n",
            "[5 / 50] Train: Loss = 4.75303, Accuracy = 96.65%: 100%|██████████| 572/572 [00:10<00:00, 55.76it/s]\n",
            "[5 / 50]   Val: Loss = 14.60463, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 64.35it/s]\n",
            "[6 / 50] Train: Loss = 4.51076, Accuracy = 96.81%: 100%|██████████| 572/572 [00:10<00:00, 56.24it/s]\n",
            "[6 / 50]   Val: Loss = 15.46680, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 66.60it/s]\n",
            "[7 / 50] Train: Loss = 4.30539, Accuracy = 96.92%: 100%|██████████| 572/572 [00:10<00:00, 55.89it/s]\n",
            "[7 / 50]   Val: Loss = 14.71946, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 66.60it/s]\n",
            "[8 / 50] Train: Loss = 3.90322, Accuracy = 97.10%: 100%|██████████| 572/572 [00:10<00:00, 55.00it/s]\n",
            "[8 / 50]   Val: Loss = 15.39336, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 64.53it/s]\n",
            "[9 / 50] Train: Loss = 3.97157, Accuracy = 97.15%: 100%|██████████| 572/572 [00:10<00:00, 55.88it/s]\n",
            "[9 / 50]   Val: Loss = 13.40770, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 65.48it/s]\n",
            "[10 / 50] Train: Loss = 3.47624, Accuracy = 97.33%: 100%|██████████| 572/572 [00:10<00:00, 56.24it/s]\n",
            "[10 / 50]   Val: Loss = 15.80261, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 69.26it/s]\n",
            "[11 / 50] Train: Loss = 3.30966, Accuracy = 97.44%: 100%|██████████| 572/572 [00:10<00:00, 54.83it/s]\n",
            "[11 / 50]   Val: Loss = 16.36386, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 61.81it/s]\n",
            "[12 / 50] Train: Loss = 3.20290, Accuracy = 97.48%: 100%|██████████| 572/572 [00:10<00:00, 54.81it/s]\n",
            "[12 / 50]   Val: Loss = 15.45736, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 69.01it/s]\n",
            "[13 / 50] Train: Loss = 3.07225, Accuracy = 97.59%: 100%|██████████| 572/572 [00:10<00:00, 55.85it/s]\n",
            "[13 / 50]   Val: Loss = 17.63778, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 62.93it/s]\n",
            "[14 / 50] Train: Loss = 2.87325, Accuracy = 97.68%: 100%|██████████| 572/572 [00:10<00:00, 55.92it/s]\n",
            "[14 / 50]   Val: Loss = 18.29601, Accuracy = 95.55%: 100%|██████████| 13/13 [00:00<00:00, 62.26it/s]\n",
            "[15 / 50] Train: Loss = 2.74178, Accuracy = 97.76%: 100%|██████████| 572/572 [00:10<00:00, 56.19it/s]\n",
            "[15 / 50]   Val: Loss = 17.59455, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 66.85it/s]\n",
            "[16 / 50] Train: Loss = 2.52073, Accuracy = 97.84%: 100%|██████████| 572/572 [00:10<00:00, 56.94it/s]\n",
            "[16 / 50]   Val: Loss = 17.88777, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 63.54it/s]\n",
            "[17 / 50] Train: Loss = 2.56094, Accuracy = 97.85%: 100%|██████████| 572/572 [00:10<00:00, 55.72it/s]\n",
            "[17 / 50]   Val: Loss = 16.31316, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 65.35it/s]\n",
            "[18 / 50] Train: Loss = 2.63942, Accuracy = 97.82%: 100%|██████████| 572/572 [00:10<00:00, 56.35it/s]\n",
            "[18 / 50]   Val: Loss = 18.72284, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 66.37it/s]\n",
            "[19 / 50] Train: Loss = 2.40557, Accuracy = 97.97%: 100%|██████████| 572/572 [00:10<00:00, 55.37it/s]\n",
            "[19 / 50]   Val: Loss = 16.02375, Accuracy = 95.74%: 100%|██████████| 13/13 [00:00<00:00, 65.05it/s]\n",
            "[20 / 50] Train: Loss = 2.19306, Accuracy = 98.07%: 100%|██████████| 572/572 [00:10<00:00, 56.30it/s]\n",
            "[20 / 50]   Val: Loss = 17.75032, Accuracy = 95.64%: 100%|██████████| 13/13 [00:00<00:00, 66.93it/s]\n",
            "[21 / 50] Train: Loss = 2.27096, Accuracy = 98.04%: 100%|██████████| 572/572 [00:10<00:00, 56.24it/s]\n",
            "[21 / 50]   Val: Loss = 19.16016, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 64.73it/s]\n",
            "[22 / 50] Train: Loss = 2.35099, Accuracy = 98.04%: 100%|██████████| 572/572 [00:10<00:00, 51.56it/s]\n",
            "[22 / 50]   Val: Loss = 18.50814, Accuracy = 95.55%: 100%|██████████| 13/13 [00:00<00:00, 67.74it/s]\n",
            "[23 / 50] Train: Loss = 2.13697, Accuracy = 98.15%: 100%|██████████| 572/572 [00:10<00:00, 55.74it/s]\n",
            "[23 / 50]   Val: Loss = 16.12867, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 65.64it/s]\n",
            "[24 / 50] Train: Loss = 2.24823, Accuracy = 98.10%: 100%|██████████| 572/572 [00:10<00:00, 55.49it/s]\n",
            "[24 / 50]   Val: Loss = 17.04643, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 66.23it/s]\n",
            "[25 / 50] Train: Loss = 2.12118, Accuracy = 98.15%: 100%|██████████| 572/572 [00:10<00:00, 55.50it/s]\n",
            "[25 / 50]   Val: Loss = 19.99964, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 62.40it/s]\n",
            "[26 / 50] Train: Loss = 2.14668, Accuracy = 98.15%: 100%|██████████| 572/572 [00:10<00:00, 55.30it/s]\n",
            "[26 / 50]   Val: Loss = 20.93995, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 64.86it/s]\n",
            "[27 / 50] Train: Loss = 2.19005, Accuracy = 98.16%: 100%|██████████| 572/572 [00:10<00:00, 56.93it/s]\n",
            "[27 / 50]   Val: Loss = 16.90404, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 67.68it/s]\n",
            "[28 / 50] Train: Loss = 1.97956, Accuracy = 98.25%: 100%|██████████| 572/572 [00:10<00:00, 58.12it/s]\n",
            "[28 / 50]   Val: Loss = 18.66778, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 64.40it/s]\n",
            "[29 / 50] Train: Loss = 1.99893, Accuracy = 98.23%: 100%|██████████| 572/572 [00:10<00:00, 56.22it/s]\n",
            "[29 / 50]   Val: Loss = 18.27366, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 65.25it/s]\n",
            "[30 / 50] Train: Loss = 1.93866, Accuracy = 98.30%: 100%|██████████| 572/572 [00:10<00:00, 56.43it/s]\n",
            "[30 / 50]   Val: Loss = 18.15657, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 63.92it/s]\n",
            "[31 / 50] Train: Loss = 1.84495, Accuracy = 98.33%: 100%|██████████| 572/572 [00:10<00:00, 56.22it/s]\n",
            "[31 / 50]   Val: Loss = 17.88688, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 66.33it/s]\n",
            "[32 / 50] Train: Loss = 1.95353, Accuracy = 98.32%: 100%|██████████| 572/572 [00:10<00:00, 56.86it/s]\n",
            "[32 / 50]   Val: Loss = 19.68343, Accuracy = 95.51%: 100%|██████████| 13/13 [00:00<00:00, 63.81it/s]\n",
            "[33 / 50] Train: Loss = 1.90427, Accuracy = 98.30%: 100%|██████████| 572/572 [00:09<00:00, 57.79it/s]\n",
            "[33 / 50]   Val: Loss = 17.79641, Accuracy = 95.73%: 100%|██████████| 13/13 [00:00<00:00, 66.80it/s]\n",
            "[34 / 50] Train: Loss = 1.80421, Accuracy = 98.36%: 100%|██████████| 572/572 [00:10<00:00, 56.90it/s]\n",
            "[34 / 50]   Val: Loss = 18.25579, Accuracy = 95.63%: 100%|██████████| 13/13 [00:00<00:00, 67.68it/s]\n",
            "[35 / 50] Train: Loss = 1.89224, Accuracy = 98.31%: 100%|██████████| 572/572 [00:10<00:00, 58.65it/s]\n",
            "[35 / 50]   Val: Loss = 20.73602, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 62.19it/s]\n",
            "[36 / 50] Train: Loss = 1.95977, Accuracy = 98.30%: 100%|██████████| 572/572 [00:10<00:00, 55.58it/s]\n",
            "[36 / 50]   Val: Loss = 18.76552, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 67.78it/s]\n",
            "[37 / 50] Train: Loss = 1.85893, Accuracy = 98.36%: 100%|██████████| 572/572 [00:10<00:00, 56.79it/s]\n",
            "[37 / 50]   Val: Loss = 19.70231, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 67.76it/s]\n",
            "[38 / 50] Train: Loss = 1.94493, Accuracy = 98.32%: 100%|██████████| 572/572 [00:10<00:00, 55.84it/s]\n",
            "[38 / 50]   Val: Loss = 22.05572, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 66.61it/s]\n",
            "[39 / 50] Train: Loss = 1.71144, Accuracy = 98.44%: 100%|██████████| 572/572 [00:09<00:00, 57.71it/s]\n",
            "[39 / 50]   Val: Loss = 16.77358, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 69.76it/s]\n",
            "[40 / 50] Train: Loss = 1.85112, Accuracy = 98.38%: 100%|██████████| 572/572 [00:10<00:00, 55.83it/s]\n",
            "[40 / 50]   Val: Loss = 19.45870, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 66.00it/s]\n",
            "[41 / 50] Train: Loss = 1.86125, Accuracy = 98.38%: 100%|██████████| 572/572 [00:10<00:00, 56.58it/s]\n",
            "[41 / 50]   Val: Loss = 19.24394, Accuracy = 95.66%: 100%|██████████| 13/13 [00:00<00:00, 67.18it/s]\n",
            "[42 / 50] Train: Loss = 1.86679, Accuracy = 98.39%: 100%|██████████| 572/572 [00:10<00:00, 55.13it/s]\n",
            "[42 / 50]   Val: Loss = 16.48469, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 68.26it/s]\n",
            "[43 / 50] Train: Loss = 1.70120, Accuracy = 98.45%: 100%|██████████| 572/572 [00:10<00:00, 52.98it/s]\n",
            "[43 / 50]   Val: Loss = 19.60690, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 64.75it/s]\n",
            "[44 / 50] Train: Loss = 1.75056, Accuracy = 98.43%: 100%|██████████| 572/572 [00:09<00:00, 57.21it/s]\n",
            "[44 / 50]   Val: Loss = 19.15143, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 68.84it/s]\n",
            "[45 / 50] Train: Loss = 1.77294, Accuracy = 98.45%: 100%|██████████| 572/572 [00:10<00:00, 56.69it/s]\n",
            "[45 / 50]   Val: Loss = 17.57620, Accuracy = 95.64%: 100%|██████████| 13/13 [00:00<00:00, 61.62it/s]\n",
            "[46 / 50] Train: Loss = 1.60222, Accuracy = 98.52%: 100%|██████████| 572/572 [00:10<00:00, 56.24it/s]\n",
            "[46 / 50]   Val: Loss = 17.77193, Accuracy = 95.69%: 100%|██████████| 13/13 [00:00<00:00, 67.41it/s]\n",
            "[47 / 50] Train: Loss = 1.67737, Accuracy = 98.49%: 100%|██████████| 572/572 [00:10<00:00, 56.82it/s]\n",
            "[47 / 50]   Val: Loss = 20.61561, Accuracy = 95.64%: 100%|██████████| 13/13 [00:00<00:00, 62.69it/s]\n",
            "[48 / 50] Train: Loss = 1.73370, Accuracy = 98.45%: 100%|██████████| 572/572 [00:10<00:00, 55.93it/s]\n",
            "[48 / 50]   Val: Loss = 17.52944, Accuracy = 95.72%: 100%|██████████| 13/13 [00:00<00:00, 63.62it/s]\n",
            "[49 / 50] Train: Loss = 1.76083, Accuracy = 98.45%: 100%|██████████| 572/572 [00:10<00:00, 56.87it/s]\n",
            "[49 / 50]   Val: Loss = 18.24261, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 63.62it/s]\n",
            "[50 / 50] Train: Loss = 1.69613, Accuracy = 98.49%: 100%|██████████| 572/572 [00:10<00:00, 59.27it/s]\n",
            "[50 / 50]   Val: Loss = 20.83454, Accuracy = 95.66%: 100%|██████████| 13/13 [00:00<00:00, 67.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "outputId": "dc0228f6-57a2-4f06-d5d9-eb30bdd824cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def compute_accuracy(model, data, batch_size=64):\n",
        "    \"\"\"\n",
        "    Computes accuracy on the dataset wrapped in a loader\n",
        "    \n",
        "    Returns: accuracy as a float value between 0 and 1\n",
        "    \"\"\"\n",
        "    model.eval() # Evaluation mode\n",
        "    val_accuracy = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        pred = torch.argmax(logits, dim=-1)\n",
        "        \n",
        "        mask = (y_batch != 0).float()\n",
        "        \n",
        "        correct += ((pred == y_batch).float() * mask).sum().item()\n",
        "        \n",
        "        total += mask.sum().item()        \n",
        "        \n",
        "    val_accuracy = float(correct)/total\n",
        "        \n",
        "    return val_accuracy\n",
        "\n",
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {test_ac * 100} %')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 95.70109030567806 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33uW57bTuH61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional = True)\n",
        "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #print(\"Inputs: \", inputs.shape)\n",
        "        emb = self._emb(inputs)\n",
        "        #print(\"Emb: \", emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        #print(\"Output of LSTM: \", output.shape)\n",
        "        out = self._out_layer(output)\n",
        "        #print(\"Final output\", out.shape)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCg5yJYqNCmi",
        "colab_type": "code",
        "outputId": "1fd3e04e-094e-4aa3-fa9d-f454a658aa30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    lstm_layers_count=2,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 21.06382, Accuracy = 88.76%: 100%|██████████| 572/572 [00:13<00:00, 41.25it/s]\n",
            "[1 / 50]   Val: Loss = 18.40555, Accuracy = 93.83%: 100%|██████████| 13/13 [00:00<00:00, 40.41it/s]\n",
            "[2 / 50] Train: Loss = 6.14219, Accuracy = 96.47%: 100%|██████████| 572/572 [00:13<00:00, 41.49it/s]\n",
            "[2 / 50]   Val: Loss = 14.61600, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 39.91it/s]\n",
            "[3 / 50] Train: Loss = 3.82375, Accuracy = 97.51%: 100%|██████████| 572/572 [00:13<00:00, 41.13it/s]\n",
            "[3 / 50]   Val: Loss = 13.40553, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 41.22it/s]\n",
            "[4 / 50] Train: Loss = 3.20937, Accuracy = 97.85%: 100%|██████████| 572/572 [00:14<00:00, 40.83it/s]\n",
            "[4 / 50]   Val: Loss = 12.22560, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 42.41it/s]\n",
            "[5 / 50] Train: Loss = 3.02416, Accuracy = 98.00%: 100%|██████████| 572/572 [00:13<00:00, 41.05it/s]\n",
            "[5 / 50]   Val: Loss = 17.25285, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 42.94it/s]\n",
            "[6 / 50] Train: Loss = 2.69717, Accuracy = 98.18%: 100%|██████████| 572/572 [00:13<00:00, 41.68it/s]\n",
            "[6 / 50]   Val: Loss = 14.37517, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 40.80it/s]\n",
            "[7 / 50] Train: Loss = 2.67440, Accuracy = 98.24%: 100%|██████████| 572/572 [00:13<00:00, 41.01it/s]\n",
            "[7 / 50]   Val: Loss = 12.74464, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 41.65it/s]\n",
            "[8 / 50] Train: Loss = 2.54204, Accuracy = 98.34%: 100%|██████████| 572/572 [00:13<00:00, 41.36it/s]\n",
            "[8 / 50]   Val: Loss = 12.29139, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 38.90it/s]\n",
            "[9 / 50] Train: Loss = 2.21829, Accuracy = 98.46%: 100%|██████████| 572/572 [00:13<00:00, 41.74it/s]\n",
            "[9 / 50]   Val: Loss = 13.91255, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 39.88it/s]\n",
            "[10 / 50] Train: Loss = 1.98420, Accuracy = 98.56%: 100%|██████████| 572/572 [00:13<00:00, 41.50it/s]\n",
            "[10 / 50]   Val: Loss = 16.91445, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 40.01it/s]\n",
            "[11 / 50] Train: Loss = 2.18699, Accuracy = 98.50%: 100%|██████████| 572/572 [00:13<00:00, 41.48it/s]\n",
            "[11 / 50]   Val: Loss = 15.55568, Accuracy = 96.15%: 100%|██████████| 13/13 [00:00<00:00, 38.92it/s]\n",
            "[12 / 50] Train: Loss = 1.89382, Accuracy = 98.65%: 100%|██████████| 572/572 [00:13<00:00, 41.12it/s]\n",
            "[12 / 50]   Val: Loss = 11.41267, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 42.89it/s]\n",
            "[13 / 50] Train: Loss = 1.70413, Accuracy = 98.73%: 100%|██████████| 572/572 [00:13<00:00, 41.78it/s]\n",
            "[13 / 50]   Val: Loss = 14.19033, Accuracy = 96.09%: 100%|██████████| 13/13 [00:00<00:00, 40.29it/s]\n",
            "[14 / 50] Train: Loss = 1.69893, Accuracy = 98.75%: 100%|██████████| 572/572 [00:13<00:00, 41.08it/s]\n",
            "[14 / 50]   Val: Loss = 13.88664, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 40.86it/s]\n",
            "[15 / 50] Train: Loss = 1.67681, Accuracy = 98.78%: 100%|██████████| 572/572 [00:13<00:00, 41.45it/s]\n",
            "[15 / 50]   Val: Loss = 18.77402, Accuracy = 95.95%: 100%|██████████| 13/13 [00:00<00:00, 40.59it/s]\n",
            "[16 / 50] Train: Loss = 1.64861, Accuracy = 98.76%: 100%|██████████| 572/572 [00:14<00:00, 43.03it/s]\n",
            "[16 / 50]   Val: Loss = 16.23032, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 40.11it/s]\n",
            "[17 / 50] Train: Loss = 1.48829, Accuracy = 98.85%: 100%|██████████| 572/572 [00:13<00:00, 41.56it/s]\n",
            "[17 / 50]   Val: Loss = 16.54780, Accuracy = 95.72%: 100%|██████████| 13/13 [00:00<00:00, 38.63it/s]\n",
            "[18 / 50] Train: Loss = 1.57388, Accuracy = 98.83%: 100%|██████████| 572/572 [00:14<00:00, 40.81it/s]\n",
            "[18 / 50]   Val: Loss = 13.95539, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 39.61it/s]\n",
            "[19 / 50] Train: Loss = 1.44246, Accuracy = 98.90%: 100%|██████████| 572/572 [00:14<00:00, 40.46it/s]\n",
            "[19 / 50]   Val: Loss = 14.02110, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 37.79it/s]\n",
            "[20 / 50] Train: Loss = 1.45385, Accuracy = 98.91%: 100%|██████████| 572/572 [00:14<00:00, 39.75it/s]\n",
            "[20 / 50]   Val: Loss = 14.11145, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 41.84it/s]\n",
            "[21 / 50] Train: Loss = 1.32453, Accuracy = 98.96%: 100%|██████████| 572/572 [00:14<00:00, 39.70it/s]\n",
            "[21 / 50]   Val: Loss = 14.53929, Accuracy = 96.08%: 100%|██████████| 13/13 [00:00<00:00, 41.62it/s]\n",
            "[22 / 50] Train: Loss = 1.33426, Accuracy = 98.96%: 100%|██████████| 572/572 [00:14<00:00, 39.49it/s]\n",
            "[22 / 50]   Val: Loss = 12.89981, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 40.89it/s]\n",
            "[23 / 50] Train: Loss = 1.26089, Accuracy = 99.00%: 100%|██████████| 572/572 [00:14<00:00, 40.31it/s]\n",
            "[23 / 50]   Val: Loss = 12.50764, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 40.47it/s]\n",
            "[24 / 50] Train: Loss = 1.35515, Accuracy = 98.95%: 100%|██████████| 572/572 [00:13<00:00, 42.67it/s]\n",
            "[24 / 50]   Val: Loss = 15.41436, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 39.84it/s]\n",
            "[25 / 50] Train: Loss = 1.37906, Accuracy = 98.95%: 100%|██████████| 572/572 [00:14<00:00, 40.31it/s]\n",
            "[25 / 50]   Val: Loss = 15.06082, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 39.93it/s]\n",
            "[26 / 50] Train: Loss = 1.23851, Accuracy = 99.01%: 100%|██████████| 572/572 [00:13<00:00, 41.02it/s]\n",
            "[26 / 50]   Val: Loss = 17.26374, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 42.25it/s]\n",
            "[27 / 50] Train: Loss = 1.36410, Accuracy = 98.99%: 100%|██████████| 572/572 [00:14<00:00, 40.64it/s]\n",
            "[27 / 50]   Val: Loss = 16.10199, Accuracy = 95.76%: 100%|██████████| 13/13 [00:00<00:00, 39.33it/s]\n",
            "[28 / 50] Train: Loss = 1.22503, Accuracy = 99.03%: 100%|██████████| 572/572 [00:14<00:00, 38.75it/s]\n",
            "[28 / 50]   Val: Loss = 15.58183, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 41.59it/s]\n",
            "[29 / 50] Train: Loss = 1.15182, Accuracy = 99.08%: 100%|██████████| 572/572 [00:14<00:00, 39.90it/s]\n",
            "[29 / 50]   Val: Loss = 15.92248, Accuracy = 96.14%: 100%|██████████| 13/13 [00:00<00:00, 39.28it/s]\n",
            "[30 / 50] Train: Loss = 1.26396, Accuracy = 99.04%: 100%|██████████| 572/572 [00:14<00:00, 40.36it/s]\n",
            "[30 / 50]   Val: Loss = 16.53195, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 39.78it/s]\n",
            "[31 / 50] Train: Loss = 1.17766, Accuracy = 99.06%: 100%|██████████| 572/572 [00:14<00:00, 40.57it/s]\n",
            "[31 / 50]   Val: Loss = 18.57451, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 41.16it/s]\n",
            "[32 / 50] Train: Loss = 1.05275, Accuracy = 99.14%: 100%|██████████| 572/572 [00:14<00:00, 40.66it/s]\n",
            "[32 / 50]   Val: Loss = 15.99229, Accuracy = 95.95%: 100%|██████████| 13/13 [00:00<00:00, 39.49it/s]\n",
            "[33 / 50] Train: Loss = 1.16190, Accuracy = 99.08%: 100%|██████████| 572/572 [00:13<00:00, 40.96it/s]\n",
            "[33 / 50]   Val: Loss = 12.90503, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 41.48it/s]\n",
            "[34 / 50] Train: Loss = 1.26378, Accuracy = 99.03%: 100%|██████████| 572/572 [00:14<00:00, 40.70it/s]\n",
            "[34 / 50]   Val: Loss = 17.66888, Accuracy = 96.08%: 100%|██████████| 13/13 [00:00<00:00, 41.02it/s]\n",
            "[35 / 50] Train: Loss = 1.05264, Accuracy = 99.13%: 100%|██████████| 572/572 [00:14<00:00, 40.77it/s]\n",
            "[35 / 50]   Val: Loss = 18.51850, Accuracy = 95.99%: 100%|██████████| 13/13 [00:00<00:00, 38.84it/s]\n",
            "[36 / 50] Train: Loss = 1.02829, Accuracy = 99.15%: 100%|██████████| 572/572 [00:14<00:00, 40.54it/s]\n",
            "[36 / 50]   Val: Loss = 17.14875, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 39.64it/s]\n",
            "[37 / 50] Train: Loss = 1.13126, Accuracy = 99.10%: 100%|██████████| 572/572 [00:14<00:00, 40.36it/s]\n",
            "[37 / 50]   Val: Loss = 16.13927, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 39.78it/s]\n",
            "[38 / 50] Train: Loss = 1.20576, Accuracy = 99.04%: 100%|██████████| 572/572 [00:14<00:00, 41.74it/s]\n",
            "[38 / 50]   Val: Loss = 13.21117, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 41.05it/s]\n",
            "[39 / 50] Train: Loss = 1.17129, Accuracy = 99.08%: 100%|██████████| 572/572 [00:14<00:00, 40.53it/s]\n",
            "[39 / 50]   Val: Loss = 16.77586, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 39.75it/s]\n",
            "[40 / 50] Train: Loss = 1.08869, Accuracy = 99.14%: 100%|██████████| 572/572 [00:14<00:00, 40.49it/s]\n",
            "[40 / 50]   Val: Loss = 16.57501, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 38.94it/s]\n",
            "[41 / 50] Train: Loss = 0.88397, Accuracy = 99.23%: 100%|██████████| 572/572 [00:13<00:00, 41.00it/s]\n",
            "[41 / 50]   Val: Loss = 17.59462, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 40.01it/s]\n",
            "[42 / 50] Train: Loss = 1.19250, Accuracy = 99.09%: 100%|██████████| 572/572 [00:14<00:00, 40.52it/s]\n",
            "[42 / 50]   Val: Loss = 16.19978, Accuracy = 95.75%: 100%|██████████| 13/13 [00:00<00:00, 42.37it/s]\n",
            "[43 / 50] Train: Loss = 1.15390, Accuracy = 99.10%: 100%|██████████| 572/572 [00:14<00:00, 40.67it/s]\n",
            "[43 / 50]   Val: Loss = 15.89544, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 40.18it/s]\n",
            "[44 / 50] Train: Loss = 1.05197, Accuracy = 99.15%: 100%|██████████| 572/572 [00:13<00:00, 40.94it/s]\n",
            "[44 / 50]   Val: Loss = 16.20858, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 40.68it/s]\n",
            "[45 / 50] Train: Loss = 1.03762, Accuracy = 99.17%: 100%|██████████| 572/572 [00:13<00:00, 41.12it/s]\n",
            "[45 / 50]   Val: Loss = 13.11829, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 42.44it/s]\n",
            "[46 / 50] Train: Loss = 1.08855, Accuracy = 99.15%: 100%|██████████| 572/572 [00:14<00:00, 45.41it/s]\n",
            "[46 / 50]   Val: Loss = 14.51496, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 41.21it/s]\n",
            "[47 / 50] Train: Loss = 1.07216, Accuracy = 99.16%: 100%|██████████| 572/572 [00:14<00:00, 40.50it/s]\n",
            "[47 / 50]   Val: Loss = 14.13201, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 40.77it/s]\n",
            "[48 / 50] Train: Loss = 1.03389, Accuracy = 99.18%: 100%|██████████| 572/572 [00:14<00:00, 40.41it/s]\n",
            "[48 / 50]   Val: Loss = 13.62474, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 40.16it/s]\n",
            "[49 / 50] Train: Loss = 1.03870, Accuracy = 99.18%: 100%|██████████| 572/572 [00:14<00:00, 40.02it/s]\n",
            "[49 / 50]   Val: Loss = 14.80157, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 41.00it/s]\n",
            "[50 / 50] Train: Loss = 1.11127, Accuracy = 99.12%: 100%|██████████| 572/572 [00:13<00:00, 41.26it/s]\n",
            "[50 / 50]   Val: Loss = 16.27659, Accuracy = 96.08%: 100%|██████████| 13/13 [00:00<00:00, 41.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ZElGPxQP9b",
        "colab_type": "code",
        "outputId": "fc74ab98-df4a-40fa-cc29-8c013e5e3190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy for biderectional LSTM is {test_ac * 100} %')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for biderectional LSTM is 96.15415070110753 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "outputId": "531c530d-36bd-4d5a-b47a-c3cc17b4ced8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "outputId": "32d729e9-7cb3-4087-f689-e86b4240dd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding.from_pretrained(embeddings)\n",
        "        self._lstm = nn.LSTM(embeddings.shape[1], \n",
        "                             lstm_hidden_dim, \n",
        "                             num_layers=lstm_layers_count, \n",
        "                             bidirectional = True)\n",
        "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #print(\"Inputs: \", inputs.shape)\n",
        "        emb = self._emb(inputs)\n",
        "        #print(\"Emb: \", emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        #print(\"Output of LSTM: \", output.shape)\n",
        "        out = self._out_layer(output)\n",
        "        #print(\"Final output\", out.shape)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "outputId": "788719fa-dc20-47d4-c8aa-d47652af420b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    lstm_layers_count=2,\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 23.22757, Accuracy = 88.23%: 100%|██████████| 572/572 [00:12<00:00, 45.28it/s]\n",
            "[1 / 50]   Val: Loss = 16.53808, Accuracy = 94.63%: 100%|██████████| 13/13 [00:00<00:00, 55.11it/s]\n",
            "[2 / 50] Train: Loss = 7.75599, Accuracy = 95.82%: 100%|██████████| 572/572 [00:12<00:00, 45.66it/s]\n",
            "[2 / 50]   Val: Loss = 11.20177, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 60.20it/s]\n",
            "[3 / 50] Train: Loss = 5.72953, Accuracy = 96.65%: 100%|██████████| 572/572 [00:12<00:00, 47.02it/s]\n",
            "[3 / 50]   Val: Loss = 12.87831, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 52.38it/s]\n",
            "[4 / 50] Train: Loss = 4.68853, Accuracy = 97.09%: 100%|██████████| 572/572 [00:12<00:00, 45.49it/s]\n",
            "[4 / 50]   Val: Loss = 10.64371, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 56.02it/s]\n",
            "[5 / 50] Train: Loss = 3.99593, Accuracy = 97.38%: 100%|██████████| 572/572 [00:12<00:00, 45.35it/s]\n",
            "[5 / 50]   Val: Loss = 11.19658, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 54.81it/s]\n",
            "[6 / 50] Train: Loss = 3.94515, Accuracy = 97.39%: 100%|██████████| 572/572 [00:12<00:00, 45.75it/s]\n",
            "[6 / 50]   Val: Loss = 10.08670, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 61.98it/s]\n",
            "[7 / 50] Train: Loss = 3.33616, Accuracy = 97.64%: 100%|██████████| 572/572 [00:12<00:00, 46.10it/s]\n",
            "[7 / 50]   Val: Loss = 9.89408, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 54.91it/s]\n",
            "[8 / 50] Train: Loss = 3.25733, Accuracy = 97.69%: 100%|██████████| 572/572 [00:12<00:00, 46.01it/s]\n",
            "[8 / 50]   Val: Loss = 10.65365, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 56.61it/s]\n",
            "[9 / 50] Train: Loss = 2.93311, Accuracy = 97.82%: 100%|██████████| 572/572 [00:12<00:00, 46.47it/s]\n",
            "[9 / 50]   Val: Loss = 10.59390, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 56.12it/s]\n",
            "[10 / 50] Train: Loss = 3.17426, Accuracy = 97.70%: 100%|██████████| 572/572 [00:12<00:00, 44.86it/s]\n",
            "[10 / 50]   Val: Loss = 9.15451, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 52.69it/s]\n",
            "[11 / 50] Train: Loss = 2.71620, Accuracy = 97.94%: 100%|██████████| 572/572 [00:12<00:00, 45.62it/s]\n",
            "[11 / 50]   Val: Loss = 10.15589, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 54.55it/s]\n",
            "[12 / 50] Train: Loss = 2.87960, Accuracy = 97.86%: 100%|██████████| 572/572 [00:12<00:00, 46.06it/s]\n",
            "[12 / 50]   Val: Loss = 11.99085, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 54.53it/s]\n",
            "[13 / 50] Train: Loss = 2.47371, Accuracy = 98.05%: 100%|██████████| 572/572 [00:12<00:00, 45.97it/s]\n",
            "[13 / 50]   Val: Loss = 10.81346, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 54.59it/s]\n",
            "[14 / 50] Train: Loss = 2.67838, Accuracy = 97.98%: 100%|██████████| 572/572 [00:12<00:00, 46.18it/s]\n",
            "[14 / 50]   Val: Loss = 11.52211, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 55.23it/s]\n",
            "[15 / 50] Train: Loss = 2.39810, Accuracy = 98.09%: 100%|██████████| 572/572 [00:12<00:00, 45.68it/s]\n",
            "[15 / 50]   Val: Loss = 9.20307, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 58.55it/s]\n",
            "[16 / 50] Train: Loss = 2.17659, Accuracy = 98.21%: 100%|██████████| 572/572 [00:12<00:00, 45.56it/s]\n",
            "[16 / 50]   Val: Loss = 10.20720, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 58.01it/s]\n",
            "[17 / 50] Train: Loss = 2.76896, Accuracy = 97.91%: 100%|██████████| 572/572 [00:12<00:00, 45.92it/s]\n",
            "[17 / 50]   Val: Loss = 9.05251, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 60.01it/s]\n",
            "[18 / 50] Train: Loss = 2.25167, Accuracy = 98.17%: 100%|██████████| 572/572 [00:12<00:00, 47.16it/s]\n",
            "[18 / 50]   Val: Loss = 10.43862, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 56.40it/s]\n",
            "[19 / 50] Train: Loss = 2.07887, Accuracy = 98.27%: 100%|██████████| 572/572 [00:12<00:00, 46.06it/s]\n",
            "[19 / 50]   Val: Loss = 9.26866, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 53.90it/s]\n",
            "[20 / 50] Train: Loss = 2.59770, Accuracy = 97.99%: 100%|██████████| 572/572 [00:12<00:00, 45.67it/s]\n",
            "[20 / 50]   Val: Loss = 10.09352, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 55.09it/s]\n",
            "[21 / 50] Train: Loss = 2.19546, Accuracy = 98.23%: 100%|██████████| 572/572 [00:12<00:00, 45.74it/s]\n",
            "[21 / 50]   Val: Loss = 11.28100, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 52.61it/s]\n",
            "[22 / 50] Train: Loss = 2.19340, Accuracy = 98.22%: 100%|██████████| 572/572 [00:12<00:00, 45.69it/s]\n",
            "[22 / 50]   Val: Loss = 9.30494, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 57.90it/s]\n",
            "[23 / 50] Train: Loss = 2.04348, Accuracy = 98.29%: 100%|██████████| 572/572 [00:12<00:00, 45.67it/s]\n",
            "[23 / 50]   Val: Loss = 9.54914, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 53.88it/s]\n",
            "[24 / 50] Train: Loss = 1.92864, Accuracy = 98.34%: 100%|██████████| 572/572 [00:12<00:00, 47.23it/s]\n",
            "[24 / 50]   Val: Loss = 9.50176, Accuracy = 97.09%: 100%|██████████| 13/13 [00:00<00:00, 55.61it/s]\n",
            "[25 / 50] Train: Loss = 2.47591, Accuracy = 98.08%: 100%|██████████| 572/572 [00:12<00:00, 45.86it/s]\n",
            "[25 / 50]   Val: Loss = 9.99780, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 57.79it/s]\n",
            "[26 / 50] Train: Loss = 2.32787, Accuracy = 98.15%: 100%|██████████| 572/572 [00:12<00:00, 45.02it/s]\n",
            "[26 / 50]   Val: Loss = 9.55247, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 58.94it/s]\n",
            "[27 / 50] Train: Loss = 1.90271, Accuracy = 98.38%: 100%|██████████| 572/572 [00:12<00:00, 46.26it/s]\n",
            "[27 / 50]   Val: Loss = 10.81147, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 49.98it/s]\n",
            "[28 / 50] Train: Loss = 1.67314, Accuracy = 98.51%: 100%|██████████| 572/572 [00:12<00:00, 45.94it/s]\n",
            "[28 / 50]   Val: Loss = 9.36311, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 57.32it/s]\n",
            "[29 / 50] Train: Loss = 2.43443, Accuracy = 98.10%: 100%|██████████| 572/572 [00:12<00:00, 44.43it/s]\n",
            "[29 / 50]   Val: Loss = 9.03400, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 57.28it/s]\n",
            "[30 / 50] Train: Loss = 2.38388, Accuracy = 98.12%: 100%|██████████| 572/572 [00:12<00:00, 45.79it/s]\n",
            "[30 / 50]   Val: Loss = 10.32682, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 52.00it/s]\n",
            "[31 / 50] Train: Loss = 1.97439, Accuracy = 98.35%: 100%|██████████| 572/572 [00:12<00:00, 45.36it/s]\n",
            "[31 / 50]   Val: Loss = 9.70181, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 59.51it/s]\n",
            "[32 / 50] Train: Loss = 1.80916, Accuracy = 98.43%: 100%|██████████| 572/572 [00:12<00:00, 45.97it/s]\n",
            "[32 / 50]   Val: Loss = 9.43999, Accuracy = 97.21%: 100%|██████████| 13/13 [00:00<00:00, 58.73it/s]\n",
            "[33 / 50] Train: Loss = 2.01670, Accuracy = 98.35%: 100%|██████████| 572/572 [00:12<00:00, 45.15it/s]\n",
            "[33 / 50]   Val: Loss = 10.61597, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 55.82it/s]\n",
            "[34 / 50] Train: Loss = 2.62964, Accuracy = 98.05%: 100%|██████████| 572/572 [00:12<00:00, 45.73it/s]\n",
            "[34 / 50]   Val: Loss = 12.41920, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 55.47it/s]\n",
            "[35 / 50] Train: Loss = 1.87587, Accuracy = 98.39%: 100%|██████████| 572/572 [00:12<00:00, 45.36it/s]\n",
            "[35 / 50]   Val: Loss = 7.67130, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 58.14it/s]\n",
            "[36 / 50] Train: Loss = 1.62129, Accuracy = 98.55%: 100%|██████████| 572/572 [00:12<00:00, 45.54it/s]\n",
            "[36 / 50]   Val: Loss = 9.36998, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 54.58it/s]\n",
            "[37 / 50] Train: Loss = 1.55221, Accuracy = 98.59%: 100%|██████████| 572/572 [00:12<00:00, 43.71it/s]\n",
            "[37 / 50]   Val: Loss = 10.46238, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 55.06it/s]\n",
            "[38 / 50] Train: Loss = 2.42378, Accuracy = 98.14%: 100%|██████████| 572/572 [00:12<00:00, 45.78it/s]\n",
            "[38 / 50]   Val: Loss = 9.63269, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 55.61it/s]\n",
            "[39 / 50] Train: Loss = 2.40169, Accuracy = 98.13%: 100%|██████████| 572/572 [00:12<00:00, 45.67it/s]\n",
            "[39 / 50]   Val: Loss = 11.32791, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 55.41it/s]\n",
            "[40 / 50] Train: Loss = 1.85702, Accuracy = 98.43%: 100%|██████████| 572/572 [00:12<00:00, 43.62it/s]\n",
            "[40 / 50]   Val: Loss = 9.38773, Accuracy = 97.18%: 100%|██████████| 13/13 [00:00<00:00, 55.26it/s]\n",
            "[41 / 50] Train: Loss = 1.48711, Accuracy = 98.63%: 100%|██████████| 572/572 [00:12<00:00, 45.69it/s]\n",
            "[41 / 50]   Val: Loss = 9.39217, Accuracy = 97.13%: 100%|██████████| 13/13 [00:00<00:00, 54.19it/s]\n",
            "[42 / 50] Train: Loss = 1.74570, Accuracy = 98.49%: 100%|██████████| 572/572 [00:12<00:00, 45.85it/s]\n",
            "[42 / 50]   Val: Loss = 11.62495, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 55.79it/s]\n",
            "[43 / 50] Train: Loss = 2.41068, Accuracy = 98.13%: 100%|██████████| 572/572 [00:12<00:00, 45.76it/s]\n",
            "[43 / 50]   Val: Loss = 10.83453, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 51.84it/s]\n",
            "[44 / 50] Train: Loss = 2.24305, Accuracy = 98.21%: 100%|██████████| 572/572 [00:12<00:00, 45.74it/s]\n",
            "[44 / 50]   Val: Loss = 10.22184, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 56.17it/s]\n",
            "[45 / 50] Train: Loss = 1.94430, Accuracy = 98.36%: 100%|██████████| 572/572 [00:12<00:00, 45.61it/s]\n",
            "[45 / 50]   Val: Loss = 9.93244, Accuracy = 97.10%: 100%|██████████| 13/13 [00:00<00:00, 52.74it/s]\n",
            "[46 / 50] Train: Loss = 1.55745, Accuracy = 98.60%: 100%|██████████| 572/572 [00:12<00:00, 46.35it/s]\n",
            "[46 / 50]   Val: Loss = 9.72822, Accuracy = 97.06%: 100%|██████████| 13/13 [00:00<00:00, 56.82it/s]\n",
            "[47 / 50] Train: Loss = 2.00711, Accuracy = 98.35%: 100%|██████████| 572/572 [00:12<00:00, 47.02it/s]\n",
            "[47 / 50]   Val: Loss = 10.29195, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 53.64it/s]\n",
            "[48 / 50] Train: Loss = 2.62307, Accuracy = 97.99%: 100%|██████████| 572/572 [00:12<00:00, 45.59it/s]\n",
            "[48 / 50]   Val: Loss = 9.78356, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 56.71it/s]\n",
            "[49 / 50] Train: Loss = 1.75813, Accuracy = 98.45%: 100%|██████████| 572/572 [00:12<00:00, 45.46it/s]\n",
            "[49 / 50]   Val: Loss = 9.74561, Accuracy = 97.20%: 100%|██████████| 13/13 [00:00<00:00, 52.09it/s]\n",
            "[50 / 50] Train: Loss = 1.44675, Accuracy = 98.67%: 100%|██████████| 572/572 [00:12<00:00, 44.83it/s]\n",
            "[50 / 50]   Val: Loss = 9.71926, Accuracy = 97.19%: 100%|██████████| 13/13 [00:00<00:00, 57.10it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "outputId": "2596d9af-3532-4dac-8cb9-43a7f69dd246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy for biderectional LSTM with pretrained embeddings is {test_ac * 100} %')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for biderectional LSTM with pretrained embeddings is 97.23481679628463 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}